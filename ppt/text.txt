page1:
I'm Hua Guo from Renmin University of China.
I would introduce our work "Lock Violation for Fault-tolerant Distributed Database System".
The co-authors has Xuan Zhou from East China Normal University and Le Cai from Alibaba Group.

page2:
Let's first look at the modern geo-replicated distributed database.
This figure shows the architecture.
The database scales horizontally by sharding their data across nodes.
It builds its transactional layers on a replication layer,
The replication layer replicated the log to multiple available-zones to achieve fault-tolerance.
It also employs a consensus protocol to ensure data consistency.
Synchronization among replicated state machines thus becomes a significant overhead of transaction processing.

page3.1£º
We want to optimized geo-replicated distributed database using two-phase lock and two phase commit.
A transaction commit needs a lot of chatty messages during the commit.
And most of the message would be the wide-area network message.
This picture shows the message flow and lock duration.
The two-phase procedure and the consensus protocol amplify lock duration and critical path.

page3.2:
The red line in this picture shows the critical path.
We know that there is a gap between wide-area network message latency and local area network message latency.
For local area network message, the latency is zero point one to zero point two milliseconds
And the wide-area network message has a latency is about thirty to two hundred milliseconds.
Most time, the two-phase endures no wide-area network latency if the replica leader is in the same availabile zones.

page4:
Our goal is to shorten the critical path and increase concurrency when encounter locks conflict.
Most important, we must keep transaction and replication correctness and achieve performance.

page5:
The basic idea here is to consolidate many lock wait to a final one wait during distributed commit.
The first line in this picture shows the normal case using locks; 
the transaction must be blocked when encountering conflict,
it cannot process further access.
And the second line shows the case using lock violation,
a transaction needs no wait and can execute later operations.
This idea of exploiting speculation is not new.

page6:
The most related work of ours is  Controlled Lock Violation of Goetz Graefe et al..
We extend CLV to a new application environment, geo-replicated distributed database, 
and we call it distributed lock violation(abbreviated as DLV).
We want to know the proper violation time, how DLV adapts to the deadlock handling technique, and so on. 
This is what CLV did not tell us.

page7.1:
We first want to get the correct criteria for lock violation.
We have known that serializability is a correct criterion for transaction processing.
But serializability only tells concurrency control correctness.
Strictness is another correct criterion relating to recovery.
Most databases choose strictness.
However, strictness is not necessarily for correctness.
We choose more loose criteria, recoverability.

page7.2
This figure shows a non-strict but recoverable schedule.
Transaction 1, 2, and 3 access,row x, y, z.
And x, y,z are on nodes S1, S2, S3, 
Transaction 2 read an uncommitted write value of transaction 1.
Transaction 1 commits ahead of transaction 2, and this schedule is recoverable.

page8:
We have several design considerations for DLV.
First, we want to know the proper time to enable lock violation.
Second, how we maintain the dependency.
Third, how to combine lock violation with deadlock handling.
Fourth, how storage affects lock violation's recoverability.
We describe the first two here, and more would be found in our paper.

page9:
We consider several time points enabling lock violation. 
The first is DLV0, which immediately enable violation after an access operation;
The second is DLV1, in the first phase of 2PC.
The third is DLV2, in the second phase of 2PC.
The fourth DLV1x, after getting a certification of a transaction's global serializability.
The fourth is a special optimize for DLV1.
which the transaction cannot get the information if it can ensure global serializability when in the first phase of two-phases.
It can prevent a lot of possible aborts.
We call that DLV0, DLV1 early violation,
and DLV2, DLV1x late violation.

page10:
Dependency maintenance is another important considerations.
We use a register and report technique to trace dependencies.
Each transaction RM maintains an in counter and an outer set.
The in-counter records the number of transactions depends on it.
The outer set records all the transaction who has a dependency on this RM.
When a transaction T violates another transaction S's lock,
RM registers the commit dependency,
by adding T to the corresponding outer set of S,
and incrementing the corresponding in of T by one.

page11:
An RM will not prepare commit if its in-dependency counter is greater than 0.
If a transaction commits, 
it reduces the in counter value by 1 of all transactions in its outer set, 
and test whose in counter value equals to 0.
If it has, it will stop waiting for its dependencies.
If a transaction aborts, 
all its out dependency transaction in outer set must abort, and this causes cascade abort.
Notice that not all dependencies, are necessary for all violation time.

page12.1:
In the following content, we have our evaluations.
We use 4 shards and each shard has 3 replicas.
We evaluate the impact of deadlock handling, replication latency, and contention.
Other results can be found in our paper, including cascade abort, transaction latency.
We first evaluate the YCSB benchmark, the skewness is set by 0.2
On the left, we show the result of YCSB performance when increasing terminal numbers.
Deadlock detection and deadlock prevention are both shown.

page12.2:
Deadlock detection can be expensive when there is a high degree of contention and high concurrency.
In contrast, deadlock prevention will incur a high abort rate for early lock violation.
For performance reasons,  we use deadlock detection for the early violation
and deadlock prevention for late violation in the following evaluation.
On the right-hand side, we show the impact of replication latency.
DLV outperformed S2PL when the replication latency is high.

page13:
This page shows the TPC-C benchmark evaluation.
We evaluate the impact of contention by adding hotspot rows.
When the degree of contention increased, the throughput decreased quickly.
The DLVs performed better than strict two-phase lock in general, 
especially when the degree of contention is high.
We also evaluate the scalability of DLV.
DLVs outperformed S2PL.
And we found that 
late violation performs better than early violation on scalability.

page14£º
To evaluate the cost of cascade abort on different lock violation approaches, we conducted additional experiments.
in which the number of warehouses was fixed to 40  and the terminal number to 200.
Left figure shows the abort rates under normal TPC-C workload.
Right shows the abort rates under a modified TPC-C workload, 
in which each transaction has 10% of the chance to access a non-existent row and incur an abort.  
As we can see, S2PL, DLV2, and DLV1x seldom encounter cascade aborts, 
no matter whether deadlock detection or deadlock prevention is applied. 
There is a significant chance of cascade abort for DLV1 and DLV0, 
especially when deadlock detection is applied. 
The total abort rates incurred by deadlock prevention were always higher than those by deadlock detection.

page15:
These figures show transaction latency evaluation.
During these experiments, 
a transaction would be immediately retried after it was abort.
The average execution time of committed transactions was calculated as the transaction latency.
We can see that the transaction latency is mainly affected by the deadlock handling cost and the workload.
In general, deadlock prevention incurs less overhead and results in better latency than deadlock detection. 
If the same deadlock handling method is applied and the workload is fixed, 
the average transaction latencies of different approaches are actually similar. 
DLV does not have a direct impact on transaction latency.
Although DLV can reduce the chance of blocking, a transaction still has to wait for all its preceding transactions to finish before it can commit.

page16:
Finally, we can draw our conclusion that:
Geo-replication could have a big negative impact on performance.
Lock violation could be an effective approach to shorten the critical path and boost performance.
It is important to find the right time to violate the locks.
When there are more idle resources, 
it is more rewarding to use aggressive lock violation and deadlock detection to avoid false-positive abort.
In contrast, when a system reaches its saturation point, 
the system should switch to a more conservative lock violation to avoid its overheads.

page17:
Thank you for listening, and I am willing to answer any questions.